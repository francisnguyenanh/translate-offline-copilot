# -*- coding: utf-8 -*-
"""
·ª®ng d·ª•ng Flask qu·∫£n l√Ω tr√≠ch xu·∫•t v√† n·∫°p b·∫£n d·ªãch cho file Excel, PowerPoint v√† Word
"""

import os
import json
import zipfile
import uuid
import shutil
import io
import re
from datetime import datetime, timedelta
from urllib.parse import quote
from flask import Flask, render_template, request, send_file, jsonify, session, redirect, url_for
from werkzeug.utils import secure_filename
from copy import deepcopy
from openpyxl import load_workbook
from pptx import Presentation
from docx import Document
from functools import wraps
from lxml import etree as _etree

# Kh·ªüi t·∫°o ·ª©ng d·ª•ng Flask
app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 60 * 1024 * 1024  # Gi·ªõi h·∫°n 50MB
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['SECRET_KEY'] = os.urandom(24)  # Secret key cho session
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(hours=5)  # Session timeout 5h

# C√°c ƒë·ªãnh d·∫°ng file ƒë∆∞·ª£c ph√©p
ALLOWED_EXTENSIONS = {'xlsx', 'pptx', 'docx'}

# ƒê·ªçc password t·ª´ file
PASSWORD_FILE = 'password.txt'

# File l∆∞u Prompt Templates
TEMPLATES_FILE = 'prompt_templates.json'

# ==================== HELPER: PROMPT TEMPLATES ====================
def get_default_templates():
    """Tr·∫£ v·ªÅ danh s√°ch template m·∫∑c ƒë·ªãnh"""
    return [
        {
            "id": "formal",
            "name": "D·ªãch ch√≠nh x√°c (Formal)",
            "content": "H√£y d·ªãch c√°c gi√° tr·ªã (values) trong file JSON n√†y sang {TARGET_LANG}.\n\nPhong c√°ch: Ch√≠nh x√°c, chuy√™n nghi·ªáp, d√πng trong t√†i li·ªáu kinh doanh.\n\nQuy t·∫Øc b·∫Øt bu·ªôc:\n1. Gi·ªØ nguy√™n 100% c√°c keys\n2. CH·ªà d·ªãch n·ªôi dung b√™n trong values\n3. KH√îNG d·ªãch t·ª´/c·ª•m t·ª´ ƒë√£ l√† ng√¥n ng·ªØ ƒë√≠ch\n4. KH√îNG d·ªãch m√£ k·ªπ thu·∫≠t, placeholder, t√™n bi·∫øn\n5. KH√îNG d·ªãch s·ªë, ng√†y th√°ng, k√Ω hi·ªáu ƒë·∫∑c bi·ªát\n6. Gi·ªØ nguy√™n format JSON chu·∫©n\n\n‚ö†Ô∏è QUY T·∫Æc v·ªÅ d·∫•u ngo·∫∑c k√©p: CH·ªà d√πng \" (U+0022). KH√îNG d√πng \u201c \u201d \u201e \u201f \u00ab \u00bb\nTr√≠ch d·∫´n: d√πng \u300c \u300dho·∫∑c 'ƒë∆°n'\n\nOutput: Tr·∫£ v·ªÅ ƒê√öNG c·∫•u tr√∫c JSON, KH√îNG th√™m gi·∫£i th√≠ch."
        },
        {
            "id": "casual",
            "name": "D·ªãch t·ª± nhi√™n (Casual)",
            "content": "H√£y d·ªãch c√°c gi√° tr·ªã (values) trong file JSON n√†y sang {TARGET_LANG}.\n\nPhong c√°ch: T·ª± nhi√™n, th√¢n thi·ªán, d·ªÖ ƒë·ªçc - ph√π h·ª£p cho giao di·ªán ng∆∞·ªùi d√πng.\n\nQuy t·∫Øc b·∫Øt bu·ªôc:\n1. Gi·ªØ nguy√™n 100% c√°c keys\n2. CH·ªà d·ªãch n·ªôi dung b√™n trong values\n3. KH√îNG d·ªãch t·ª´/c·ª•m t·ª´ ƒë√£ l√† ng√¥n ng·ªØ ƒë√≠ch\n4. KH√îNG d·ªãch m√£ k·ªπ thu·∫≠t, placeholder, t√™n bi·∫øn\n5. KH√îNG d·ªãch s·ªë, ng√†y th√°ng, k√Ω hi·ªáu ƒë·∫∑c bi·ªát\n6. Gi·ªØ nguy√™n format JSON chu·∫©n\n\n‚ö†Ô∏è QUY T·∫Æc v·ªÅ d·∫•u ngo·∫∑c k√©p: CH·ªà d√πng \" (U+0022). KH√îNG d√πng \u201c \u201d \u201e \u201f \u00ab \u00bb\n\nOutput: Tr·∫£ v·ªÅ ƒê√öNG c·∫•u tr√∫c JSON, KH√îNG th√™m gi·∫£i th√≠ch."
        },
        {
            "id": "technical",
            "name": "D·ªãch k·ªπ thu·∫≠t (Technical)",
            "content": "H√£y d·ªãch c√°c gi√° tr·ªã (values) trong file JSON n√†y sang {TARGET_LANG}.\n\nPhong c√°ch: K·ªπ thu·∫≠t, ch√≠nh x√°c cao, gi·ªØ nguy√™n thu·∫≠t ng·ªØ IT.\n\nQuy t·∫Øc b·∫Øt bu·ªôc:\n1. Gi·ªØ nguy√™n 100% c√°c keys\n2. CH·ªà d·ªãch n·ªôi dung b√™n trong values\n3. KH√îNG d·ªãch t·ª´/c·ª•m t·ª´ ƒë√£ l√† ng√¥n ng·ªØ ƒë√≠ch\n4. KH√îNG d·ªãch placeholder ({0}, %s, $n...), t√™n bi·∫øn\n5. KH√îNG d·ªãch s·ªë, ng√†y th√°ng, k√Ω hi·ªáu ƒë·∫∑c bi·ªát\n6. Gi·ªØ nguy√™n thu·∫≠t ng·ªØ IT ti·∫øng Anh n·∫øu kh√¥ng c√≥ t·ª´ t∆∞∆°ng ƒë∆∞∆°ng ch√≠nh x√°c\n7. Gi·ªØ nguy√™n format JSON chu·∫©n\n\n‚ö†Ô∏è QUY T·∫Æc v·ªÅ d·∫•u ngo·∫∑c k√©p: CH·ªà d√πng \" (U+0022). KH√îNG d√πng \u201c \u201d \u201e \u201f \u00ab \u00bb\n\nOutput: Tr·∫£ v·ªÅ ƒê√öNG c·∫•u tr√∫c JSON, KH√îNG th√™m gi·∫£i th√≠ch."
        }
    ]

def load_templates(lang='default'):
    """ƒê·ªçc prompt templates cho m·ªôt ng√¥n ng·ªØ c·ª• th·ªÉ (fallback v·ªÅ default)"""
    try:
        with open(TEMPLATES_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # T∆∞∆°ng th√≠ch ng∆∞·ª£c: n·∫øu data l√† array th√¨ ƒë√≥ l√† format c≈©
        if isinstance(data, list):
            return data
        return data.get(lang) or data.get('default') or get_default_templates()
    except Exception:
        return get_default_templates()


def build_dedup_data(extracted_data, chunk_size=400):
    """
    G·ªôp c√°c keys c√≥ c√πng value ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng c·∫ßn d·ªãch.
    Returns: (dedup_files, mapping, stats)
      - dedup_files: list of {name, content} ‚Äì c√°c chunk dedup (gi·ªëng format files th∆∞·ªùng)
      - mapping: {dedup_key: [orig_key1, orig_key2, ...]}
      - stats: {total, unique, saved, percent_saved}
    """
    # Group keys by value (gi·ªØ order)
    value_to_keys = {}
    for key, value in extracted_data.items():
        if value not in value_to_keys:
            value_to_keys[value] = []
        value_to_keys[value].append(key)

    # Build dedup dict v√† mapping
    dedup_data = {}
    mapping = {}  # dedup_key ‚Üí [original_keys]
    for idx, (value, keys) in enumerate(value_to_keys.items(), 1):
        dk = f'dedup_{idx}'
        dedup_data[dk] = value
        mapping[dk] = keys

    total = len(extracted_data)
    unique = len(dedup_data)
    saved = total - unique
    percent = round(saved * 100 / total) if total > 0 else 0
    stats = {'total': total, 'unique': unique, 'saved': saved, 'percent_saved': percent}

    # Chia th√†nh c√°c chunk
    items = list(dedup_data.items())
    num_chunks = max(1, (unique + chunk_size - 1) // chunk_size)
    dedup_files = []
    for i in range(num_chunks):
        chunk = dict(items[i * chunk_size:(i + 1) * chunk_size])
        dedup_files.append({
            'name': f'dedup_part{i+1:02d}_of_{num_chunks:02d}.json',
            'content': json.dumps(chunk, ensure_ascii=False, indent=2)
        })

    return dedup_files, mapping, stats


def expand_dedup_data(json_data, session_folder):
    """
    M·ªü r·ªông dedup JSON (dedup_N ‚Üí value) th√†nh keys g·ªëc d·ª±a tr√™n mapping ƒë√£ l∆∞u.
    N·∫øu kh√¥ng t√¨m th·∫•y mapping file th√¨ tr·∫£ v·ªÅ nguy√™n.
    """
    mapping_path = os.path.join(session_folder, 'dedup_mapping.json')
    if not os.path.exists(mapping_path):
        return json_data
    try:
        with open(mapping_path, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
        expanded = {}
        for key, value in json_data.items():
            if key.startswith('dedup_') and key in mapping:
                for orig_key in mapping[key]:
                    expanded[orig_key] = value
            else:
                expanded[key] = value  # key th∆∞·ªùng, gi·ªØ nguy√™n
        return expanded
    except Exception:
        return json_data


def get_password():
    """ƒê·ªçc password t·ª´ file password.txt"""
    try:
        with open(PASSWORD_FILE, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except FileNotFoundError:
        # N·∫øu file kh√¥ng t·ªìn t·∫°i, t·∫°o file v·ªõi password m·∫∑c ƒë·ªãnh
        default_password = 'admin123'
        with open(PASSWORD_FILE, 'w', encoding='utf-8') as f:
            f.write(default_password)
        return default_password

def get_machine_id():
    """L·∫•y ID m√°y (d·ª±a tr√™n UUID node)"""
    return hex(uuid.getnode())

def create_session_id():
    """T·∫°o session ID d·ª±a tr√™n machine ID + timestamp"""
    machine_id = get_machine_id()
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    return f"{machine_id}_{timestamp}"

def get_session_folder():
    """L·∫•y ƒë∆∞·ªùng d·∫´n folder c·ªßa session hi·ªán t·∫°i"""
    if 'session_id' not in session:
        session['session_id'] = create_session_id()
    
    session_folder = os.path.join(app.config['UPLOAD_FOLDER'], session['session_id'])
    os.makedirs(session_folder, exist_ok=True)
    return session_folder

def cleanup_old_sessions():
    """X√≥a t·∫•t c·∫£ folder c·ªßa c√°c phi√™n t·ª´ h√¥m qua tr·ªü v·ªÅ tr∆∞·ªõc"""
    try:
        upload_folder = app.config['UPLOAD_FOLDER']
        if not os.path.exists(upload_folder):
            return
        
        # L·∫•y ng√†y hi·ªán t·∫°i (kh√¥ng c√≥ gi·ªù ph√∫t gi√¢y)
        today = datetime.now().date()
        
        # Duy·ªát qua t·∫•t c·∫£ c√°c folder trong uploads
        for folder_name in os.listdir(upload_folder):
            folder_path = os.path.join(upload_folder, folder_name)
            
            if os.path.isdir(folder_path):
                try:
                    # Parse timestamp t·ª´ t√™n folder (format: machine_YYYYMMDD_HHMMSS)
                    parts = folder_name.split('_')
                    if len(parts) >= 2:
                        date_str = parts[-2]  # YYYYMMDD
                        folder_date = datetime.strptime(date_str, '%Y%m%d').date()
                        
                        # N·∫øu folder t·ª´ h√¥m qua tr·ªü v·ªÅ tr∆∞·ªõc, x√≥a ƒëi
                        if folder_date < today:
                            shutil.rmtree(folder_path)
                            print(f"ƒê√£ x√≥a folder c≈©: {folder_name}")
                except (ValueError, IndexError):
                    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c, b·ªè qua
                    continue
    except Exception as e:
        print(f"L·ªói khi cleanup old sessions: {e}")

def login_required(f):
    """Decorator ƒë·ªÉ y√™u c·∫ßu ƒëƒÉng nh·∫≠p"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if 'logged_in' not in session:
            # N·∫øu l√† request AJAX/JSON, tr·∫£ v·ªÅ JSON thay v√¨ redirect
            if request.path.startswith('/api') or request.is_json or request.path in ['/extract', '/inject', '/clear-uploads']:
                return jsonify({'error': 'Ch∆∞a ƒëƒÉng nh·∫≠p ho·∫∑c phi√™n ƒë√£ h·∫øt h·∫°n'}), 401
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function

def allowed_file(filename):
    """
    Ki·ªÉm tra xem file c√≥ ph·∫£i ƒë·ªãnh d·∫°ng ƒë∆∞·ª£c ph√©p kh√¥ng
    """
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def set_download_headers(response, display_name, default_ascii_name):
    """Set Content-Disposition h·ªó tr·ª£ t√™n file Unicode (RFC 5987)."""
    encoded_filename = quote(display_name, safe='')
    ascii_filename = secure_filename(display_name) or default_ascii_name
    response.headers['Content-Disposition'] = (
        f"attachment; filename=\"{ascii_filename}\"; filename*=UTF-8''{encoded_filename}"
    )
    return response

# Error Handlers
@app.errorhandler(400)
def bad_request(error):
    """X·ª≠ l√Ω l·ªói 400 Bad Request"""
    if request.path.startswith('/api') or request.is_json or request.path in ['/extract', '/inject', '/clear-uploads']:
        return jsonify({'error': str(error) or 'Y√™u c·∫ßu kh√¥ng h·ª£p l·ªá'}), 400
    return str(error), 400

@app.errorhandler(401)
def unauthorized(error):
    """X·ª≠ l√Ω l·ªói 401 Unauthorized"""
    if request.path.startswith('/api') or request.is_json or request.path in ['/extract', '/inject', '/clear-uploads']:
        return jsonify({'error': 'Ch∆∞a ƒëƒÉng nh·∫≠p ho·∫∑c phi√™n ƒë√£ h·∫øt h·∫°n'}), 401
    return redirect(url_for('login'))

@app.errorhandler(404)
def not_found(error):
    """X·ª≠ l√Ω l·ªói 404 Not Found"""
    if request.path.startswith('/api') or request.is_json:
        return jsonify({'error': 'Kh√¥ng t√¨m th·∫•y t√†i nguy√™n'}), 404
    return str(error), 404

@app.errorhandler(413)
def request_entity_too_large(error):
    """X·ª≠ l√Ω l·ªói 413 Request Entity Too Large"""
    return jsonify({'error': 'File qu√° l·ªõn. Gi·ªõi h·∫°n 50MB'}), 413

@app.errorhandler(500)
def internal_server_error(error):
    """X·ª≠ l√Ω l·ªói 500 Internal Server Error"""
    if request.path.startswith('/api') or request.is_json or request.path in ['/extract', '/inject', '/clear-uploads']:
        return jsonify({'error': f'L·ªói m√°y ch·ªß: {str(error)}'}), 500
    return str(error), 500

def extract_text_from_shape(shape, shape_path, extracted_data):
    """
    H√†m ƒë·ªá quy ƒë·ªÉ tr√≠ch xu·∫•t text t·ª´ shape, bao g·ªìm c·∫£ grouped shapes
    shape_path: ƒë∆∞·ªùng d·∫´n ƒë·∫øn shape, v√≠ d·ª• "Shape1" ho·∫∑c "Shape1_2_3"
    """
    # Tr√≠ch xu·∫•t text t·ª´ text frame c·ªßa shape hi·ªán t·∫°i
    if hasattr(shape, "text") and shape.text:
        text_content = shape.text.strip()
        if text_content:  # Ch·ªâ l·∫•y n·ªôi dung kh√¥ng r·ªóng
            extracted_data[shape_path] = text_content
    
    # Tr√≠ch xu·∫•t text t·ª´ table n·∫øu c√≥
    if hasattr(shape, "has_table") and shape.has_table:
        table = shape.table
        for row_idx, row in enumerate(table.rows, start=1):
            for col_idx, cell in enumerate(row.cells, start=1):
                if cell.text.strip():
                    key = f"{shape_path}!Table_R{row_idx}C{col_idx}"
                    extracted_data[key] = cell.text.strip()
    
    # Ki·ªÉm tra xem shape c√≥ ph·∫£i l√† GroupShape kh√¥ng (ch·ª©a c√°c shape con)
    if hasattr(shape, "shapes"):
        # ƒê√¢y l√† grouped shape, duy·ªát qua c√°c shape con
        for child_idx, child_shape in enumerate(shape.shapes, start=1):
            child_path = f"{shape_path}_{child_idx}"
            extract_text_from_shape(child_shape, child_path, extracted_data)

def extract_text_from_pptx(filepath):
    """
    Tr√≠ch xu·∫•t text t·ª´ file PPTX, bao g·ªìm c·∫£ text trong grouped shapes
    Tr·∫£ v·ªÅ dictionary v·ªõi format: {"SlideX!ShapeY": "Content"}
    V·ªõi nested shapes: {"SlideX!ShapeY_Z": "Content"} (Z l√† shape con)
    """
    extracted_data = {}
    prs = Presentation(filepath)
    
    for slide_idx, slide in enumerate(prs.slides, start=1):
        for shape_idx, shape in enumerate(slide.shapes, start=1):
            shape_path = f"Slide{slide_idx}!Shape{shape_idx}"
            extract_text_from_shape(shape, shape_path, extracted_data)
    
    return extracted_data

def inject_text_to_shape(shape, shape_indices, translated_value, is_table_cell=False, table_pos=None):
    """
    H√†m ƒë·ªá quy ƒë·ªÉ n·∫°p text v√†o shape, bao g·ªìm c·∫£ grouped shapes
    shape_indices: list c√°c index ƒë·ªÉ navigate ƒë·∫øn shape ƒë√∫ng, v√≠ d·ª• [2, 3] cho Shape2_3
    is_table_cell: c√≥ ph·∫£i l√† table cell kh√¥ng
    table_pos: tuple (row_idx, col_idx) n·∫øu l√† table cell
    """
    # N·∫øu l√† shape cu·ªëi c√πng trong path
    if len(shape_indices) == 0:
        if is_table_cell and table_pos:
            # N·∫°p v√†o table cell - gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng
            row_idx, col_idx = table_pos
            if hasattr(shape, "has_table") and shape.has_table:
                table = shape.table
                if row_idx < len(table.rows) and col_idx < len(table.rows[row_idx].cells):
                    cell = table.rows[row_idx].cells[col_idx]
                    # Thay th·∫ø text trong t·ª´ng paragraph/run ƒë·ªÉ gi·ªØ ƒë·ªãnh d·∫°ng
                    if cell.text_frame:
                        replace_text_keep_format(cell.text_frame, translated_value)
        else:
            # N·∫°p v√†o text frame c·ªßa shape - gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng
            if hasattr(shape, "text_frame") and shape.text_frame:
                replace_text_keep_format(shape.text_frame, translated_value)
        return True
    
    # Navigate ƒë·∫øn shape con
    if hasattr(shape, "shapes"):
        next_idx = shape_indices[0]
        if next_idx <= len(shape.shapes):
            child_shape = shape.shapes[next_idx - 1]  # Chuy·ªÉn t·ª´ 1-indexed sang 0-indexed
            return inject_text_to_shape(child_shape, shape_indices[1:], translated_value, is_table_cell, table_pos)
    
    return False

def replace_text_keep_format(text_frame, new_text):
    """
    Thay th·∫ø text trong text_frame nh∆∞ng gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng (font, m√†u, g·∫°ch ch√¢n, bold, italic...)
    Chi·∫øn l∆∞·ª£c:
    1. N·∫øu to√†n b·ªô text frame ch·ªâ c√≥ 1 paragraph v√† 1 run -> thay text c·ªßa run ƒë√≥
    2. N·∫øu c√≥ nhi·ªÅu runs/paragraphs -> x√≥a text c·ªßa t·∫•t c·∫£ runs, g√°n text m·ªõi v√†o run ƒë·∫ßu ti√™n v·ªõi ƒë·ªãnh d·∫°ng g·ªëc
    """
    if not text_frame.paragraphs:
        return
    
    # Thu th·∫≠p t·∫•t c·∫£ runs t·ª´ t·∫•t c·∫£ paragraphs
    all_runs = []
    for paragraph in text_frame.paragraphs:
        for run in paragraph.runs:
            all_runs.append(run)
    
    if not all_runs:
        # Kh√¥ng c√≥ run n√†o, t·∫°o m·ªõi
        if text_frame.paragraphs:
            text_frame.paragraphs[0].text = new_text
        return
    
    # L∆∞u ƒë·ªãnh d·∫°ng c·ªßa run ƒë·∫ßu ti√™n
    first_run = all_runs[0]
    
    # X√≥a text c·ªßa t·∫•t c·∫£ runs
    for run in all_runs:
        run.text = ""
    
    # G√°n text m·ªõi v√†o run ƒë·∫ßu ti√™n (gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng)
    first_run.text = new_text

def inject_text_to_pptx(filepath, json_data):
    """
    N·∫°p text ƒë√£ d·ªãch v√†o file PPTX, bao g·ªìm c·∫£ grouped shapes
    """
    prs = Presentation(filepath)
    
    for key, translated_value in json_data.items():
        try:
            # Parse key format: 
            # "SlideX!ShapeY" ho·∫∑c "SlideX!ShapeY_Z" (nested) 
            # ho·∫∑c "SlideX!ShapeY!Table_RxCy" ho·∫∑c "SlideX!ShapeY_Z!Table_RxCy"
            if '!' not in key:
                continue
            
            parts = key.split('!')
            if len(parts) < 2:
                continue
            
            # L·∫•y slide index
            slide_part = parts[0]
            if not slide_part.startswith('Slide'):
                continue
            slide_idx = int(slide_part.replace('Slide', '')) - 1
            
            if slide_idx >= len(prs.slides):
                continue
            
            slide = prs.slides[slide_idx]
            
            # Parse shape path: "Shape2" ho·∫∑c "Shape2_3_1" (nested)
            shape_part = parts[1]
            if not shape_part.startswith('Shape'):
                continue
            
            # T√°ch c√°c indices: "Shape2_3_1" -> [2, 3, 1]
            shape_str = shape_part.replace('Shape', '')
            shape_indices = [int(idx) for idx in shape_str.split('_')]
            
            # L·∫•y shape ƒë·∫ßu ti√™n (top-level shape)
            first_shape_idx = shape_indices[0] - 1  # Chuy·ªÉn sang 0-indexed
            if first_shape_idx >= len(slide.shapes):
                continue
            
            shape = slide.shapes[first_shape_idx]
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i table cell kh√¥ng
            is_table_cell = False
            table_pos = None
            
            if len(parts) == 3 and parts[2].startswith('Table_R'):
                # Parse table cell position
                is_table_cell = True
                table_part = parts[2].replace('Table_R', '').split('C')
                row_idx = int(table_part[0]) - 1
                col_idx = int(table_part[1]) - 1
                table_pos = (row_idx, col_idx)
            
            # Navigate v√† n·∫°p text (b·ªè qua index ƒë·∫ßu ti√™n v√¨ ƒë√£ l·∫•y shape r·ªìi)
            inject_text_to_shape(shape, shape_indices[1:], translated_value, is_table_cell, table_pos)
            
        except (ValueError, IndexError, AttributeError) as e:
            # B·ªè qua c√°c key kh√¥ng h·ª£p l·ªá
            continue
    
    return prs

# ==================== XLSX SHAPE / OBJECT SUPPORT ====================
# Namespaces d√πng trong drawing XML c·ªßa xlsx
_NS_XDR = 'http://schemas.openxmlformats.org/drawingml/2006/spreadsheetDrawing'
_NS_A   = 'http://schemas.openxmlformats.org/drawingml/2006/main'
_NS_R   = 'http://schemas.openxmlformats.org/officeDocument/2006/relationships'
_NS_WB  = 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'


def _xlsx_get_sheet_drawing_map(z):
    """
    T·ª´ ZipFile ƒëang m·ªü, tr·∫£ v·ªÅ dict: sheet_name ‚Üí list c√°c ƒë∆∞·ªùng d·∫´n drawing XML
    V√≠ d·ª•: {'Sheet1': ['xl/drawings/drawing1.xml']}
    """
    names_set = set(z.namelist())

    # ƒê·ªçc workbook.xml ƒë·ªÉ l·∫•y t√™n sheet v√† rId
    wb_xml = z.read('xl/workbook.xml')
    wb_root = _etree.fromstring(wb_xml)

    # ƒê·ªçc rels c·ªßa workbook ƒë·ªÉ map rId ‚Üí target file
    wb_rels_xml = z.read('xl/_rels/workbook.xml.rels')
    wb_rels_root = _etree.fromstring(wb_rels_xml)
    rid_to_target = {rel.get('Id'): rel.get('Target') for rel in wb_rels_root}

    # Thu th·∫≠p (sheet_name, sheet_path)
    sheet_info = []
    for sheet_el in wb_root.iter(f'{{{_NS_WB}}}sheet'):
        name = sheet_el.get('name')
        rid  = sheet_el.get(f'{{{_NS_R}}}id')
        target = rid_to_target.get(rid, '')
        # Chu·∫©n h√≥a path: "worksheets/sheet1.xml" ‚Üí "xl/worksheets/sheet1.xml"
        if target.startswith('../'):
            sheet_path = 'xl/' + target[3:]
        elif not target.startswith('xl/'):
            sheet_path = 'xl/' + target
        else:
            sheet_path = target
        sheet_info.append((name, sheet_path))

    result = {}
    for sheet_name, sheet_path in sheet_info:
        if sheet_path not in names_set:
            continue
        parts = sheet_path.rsplit('/', 1)
        sheet_dir  = parts[0] if len(parts) > 1 else ''
        sheet_file = parts[-1]
        rels_path  = f"{sheet_dir}/_rels/{sheet_file}.rels"
        if rels_path not in names_set:
            continue

        rels_xml  = z.read(rels_path)
        rels_root = _etree.fromstring(rels_xml)

        drawing_paths = []
        for rel in rels_root:
            if 'drawing' in rel.get('Type', '').lower():
                tgt = rel.get('Target', '')
                if tgt.startswith('../'):
                    draw_path = 'xl/' + tgt[3:]
                elif tgt.startswith('/'):
                    draw_path = tgt.lstrip('/')
                else:
                    draw_path = f"{sheet_dir}/{tgt.lstrip('./')}"
                if draw_path in names_set:
                    drawing_paths.append(draw_path)
        if drawing_paths:
            result[sheet_name] = drawing_paths

    return result


def _collect_sp_elements(drawing_root):
    """
    Thu th·∫≠p t·∫•t c·∫£ ph·∫ßn t·ª≠ <xdr:sp> (shape/text-box) theo th·ª© t·ª± c√¢y (tree-order),
    bao g·ªìm c·∫£ sp b√™n trong group-shapes (xdr:grpSp).
    """
    sp_list = []
    _walk_tags = {
        f'{{{_NS_XDR}}}grpSp',
        f'{{{_NS_XDR}}}twoCellAnchor',
        f'{{{_NS_XDR}}}oneCellAnchor',
        f'{{{_NS_XDR}}}absoluteAnchor',
    }

    def walk(el):
        for child in el:
            if child.tag == f'{{{_NS_XDR}}}sp':
                sp_list.append(child)
            elif child.tag in _walk_tags:
                walk(child)

    walk(drawing_root)
    return sp_list


def _get_sp_text(sp):
    """L·∫•y to√†n b·ªô text trong txBody c·ªßa m·ªôt shape."""
    txBody = sp.find(f'{{{_NS_XDR}}}txBody')
    if txBody is None:
        return ''
    parts = []
    for para in txBody.findall(f'{{{_NS_A}}}p'):
        # L·∫•y t·∫•t c·∫£ a:t trong paragraph (bao g·ªìm c·∫£ text trong a:fld)
        para_text = ''.join((t.text or '') for t in para.findall(f'.//{{{_NS_A}}}t'))
        if para_text:
            parts.append(para_text)
    return '\n'.join(parts)


def _set_sp_text(sp, new_text):
    """
    Thay th·∫ø text trong txBody c·ªßa shape theo c√°ch t·ªëi thi·ªÉu ƒë·ªÉ t∆∞∆°ng th√≠ch Excel:
    - Gi·ªØ nguy√™n c·∫•u tr√∫c paragraph/run/fld hi·ªán c√≥
    - Ch·ªâ thay text ·ªü node a:t ƒë·∫ßu ti√™n, c√°c node a:t c√≤n l·∫°i set r·ªóng
    - N·∫øu ch∆∞a c√≥ a:t th√¨ t·∫°o m·ªõi t·ªëi thi·ªÉu trong paragraph ƒë·∫ßu
    """
    txBody = sp.find(f'{{{_NS_XDR}}}txBody')
    if txBody is None:
        return

    paras = txBody.findall(f'{{{_NS_A}}}p')
    if not paras:
        return

    all_t_nodes = txBody.findall(f'.//{{{_NS_A}}}t')
    new_text = '' if new_text is None else str(new_text)

    if all_t_nodes:
        first_t = all_t_nodes[0]
        first_t.text = new_text
        if new_text != new_text.strip() or '\n' in new_text:
            first_t.set('{http://www.w3.org/XML/1998/namespace}space', 'preserve')
        else:
            first_t.attrib.pop('{http://www.w3.org/XML/1998/namespace}space', None)

        for t in all_t_nodes[1:]:
            t.text = ''
        return

    # Kh√¥ng c√≥ a:t n√†o, t·∫°o t·ªëi thi·ªÉu trong paragraph ƒë·∫ßu
    first_para = paras[0]
    first_run = _etree.SubElement(first_para, f'{{{_NS_A}}}r')
    first_t = _etree.SubElement(first_run, f'{{{_NS_A}}}t')
    first_t.text = new_text
    if new_text != new_text.strip() or '\n' in new_text:
        first_t.set('{http://www.w3.org/XML/1998/namespace}space', 'preserve')


def extract_xlsx_shapes(filepath):
    """
    Tr√≠ch xu·∫•t text t·ª´ t·∫•t c·∫£ shape/object (text-box) trong file xlsx.
    Tr·∫£ v·ªÅ dict: {"SheetName!XLShape{n}": "text"}
    - n l√† th·ª© t·ª± shape (ƒë·∫øm T·∫§T C·∫¢ sp, bao g·ªìm c·∫£ sp kh√¥ng c√≥ text) ‚Üí index ·ªïn ƒë·ªãnh.
    """
    shapes_data = {}
    try:
        with zipfile.ZipFile(filepath, 'r') as z:
            sheet_drawing_map = _xlsx_get_sheet_drawing_map(z)
            for sheet_name, drawing_paths in sheet_drawing_map.items():
                for drawing_path in drawing_paths:
                    drawing_xml  = z.read(drawing_path)
                    drawing_root = _etree.fromstring(drawing_xml)
                    for shape_idx, sp in enumerate(_collect_sp_elements(drawing_root), start=1):
                        text = _get_sp_text(sp)
                        if text.strip():
                            key = f"{sheet_name}!XLShape{shape_idx}"
                            shapes_data[key] = text.strip()
    except Exception as e:
        print(f"Warning: Kh√¥ng th·ªÉ tr√≠ch xu·∫•t shapes t·ª´ xlsx: {e}")
    return shapes_data


def _xlsx_sheet_path_map(files):
    """Tr·∫£ v·ªÅ map: sheet_name -> sheet_xml_path t·ª´ workbook + workbook.rels."""
    workbook_root = _etree.fromstring(files['xl/workbook.xml'])
    rels_root = _etree.fromstring(files['xl/_rels/workbook.xml.rels'])
    rid_to_target = {rel.get('Id'): rel.get('Target') for rel in rels_root}

    sheet_map = {}
    for sheet_el in workbook_root.iter(f'{{{_NS_WB}}}sheet'):
        sheet_name = sheet_el.get('name')
        rid = sheet_el.get(f'{{{_NS_R}}}id')
        target = rid_to_target.get(rid, '')
        if target.startswith('/'):
            path = target.lstrip('/')
        elif target.startswith('xl/'):
            path = target
        else:
            path = f'xl/{target}'
        sheet_map[sheet_name] = path
    return sheet_map


def _xlsx_sheet_drawing_map_from_files(files, sheet_map):
    """Tr·∫£ v·ªÅ map: sheet_name -> [drawing_xml_paths]."""
    result = {}
    names_set = set(files.keys())

    for sheet_name, sheet_path in sheet_map.items():
        parts = sheet_path.rsplit('/', 1)
        sheet_dir = parts[0] if len(parts) > 1 else ''
        sheet_file = parts[-1]
        rels_path = f"{sheet_dir}/_rels/{sheet_file}.rels"
        if rels_path not in names_set:
            continue

        rels_root = _etree.fromstring(files[rels_path])
        drawing_paths = []
        for rel in rels_root:
            if 'drawing' in (rel.get('Type') or '').lower():
                tgt = rel.get('Target', '')
                if tgt.startswith('/'):
                    draw_path = tgt.lstrip('/')
                else:
                    draw_path = os.path.normpath(os.path.join(sheet_dir, tgt)).replace('\\', '/')
                if draw_path in names_set:
                    drawing_paths.append(draw_path)

        if drawing_paths:
            result[sheet_name] = drawing_paths

    return result


def _xlsx_set_cell_inline_text(sheet_root, cell_ref, text_value):
    """G√°n text v√†o m·ªôt cell theo ki·ªÉu inlineStr, gi·ªØ nguy√™n style c·ªßa cell n·∫øu c√≥."""
    m = re.fullmatch(r'([A-Za-z]+)(\d+)', cell_ref or '')
    if not m:
        return

    row_num = int(m.group(2))
    sheet_data = sheet_root.find(f'{{{_NS_WB}}}sheetData')
    if sheet_data is None:
        return

    row_elem = None
    rows = sheet_data.findall(f'{{{_NS_WB}}}row')
    for row in rows:
        if int(row.get('r', '0') or 0) == row_num:
            row_elem = row
            break

    if row_elem is None:
        row_elem = _etree.Element(f'{{{_NS_WB}}}row', r=str(row_num))
        inserted = False
        for idx, row in enumerate(rows):
            existing = int(row.get('r', '0') or 0)
            if existing > row_num:
                sheet_data.insert(idx, row_elem)
                inserted = True
                break
        if not inserted:
            sheet_data.append(row_elem)

    cell_elem = None
    for cell in row_elem.findall(f'{{{_NS_WB}}}c'):
        if (cell.get('r') or '').upper() == cell_ref.upper():
            cell_elem = cell
            break

    if cell_elem is None:
        cell_elem = _etree.Element(f'{{{_NS_WB}}}c', r=cell_ref.upper())
        row_elem.append(cell_elem)

    cell_elem.set('t', 'inlineStr')
    for child in list(cell_elem):
        if child.tag in {
            f'{{{_NS_WB}}}v',
            f'{{{_NS_WB}}}f',
            f'{{{_NS_WB}}}is',
        }:
            cell_elem.remove(child)

    is_elem = _etree.SubElement(cell_elem, f'{{{_NS_WB}}}is')
    t_elem = _etree.SubElement(is_elem, f'{{{_NS_WB}}}t')
    text_str = '' if text_value is None else str(text_value)
    t_elem.text = text_str
    if text_str != text_str.strip() or '\n' in text_str:
        t_elem.set('{http://www.w3.org/XML/1998/namespace}space', 'preserve')


def inject_xlsx_shapes(source_filepath, output_filepath, json_data):
    """
    ZIP-level patch cho XLSX:
    - Kh√¥ng d√πng openpyxl.save
    - Patch tr·ª±c ti·∫øp cell XML
    - Patch tr·ª±c ti·∫øp drawing XML (shape text)
    - Gi·ªØ nguy√™n to√†n b·ªô parts kh√°c c·ªßa file g·ªëc
    """
    with zipfile.ZipFile(source_filepath, 'r') as z_src:
        infos = {info.filename: info for info in z_src.infolist()}
        order = [info.filename for info in z_src.infolist()]
        files = {name: z_src.read(name) for name in order}

    sheet_map = _xlsx_sheet_path_map(files)

    cell_updates = {}
    shape_updates = {}
    for key, translated_value in json_data.items():
        if '!' not in key:
            continue
        sheet_name, second_part = key.split('!', 1)
        if sheet_name not in sheet_map:
            continue

        if second_part.startswith('XLShape'):
            try:
                shape_idx = int(second_part.replace('XLShape', ''))
                shape_updates[(sheet_name, shape_idx)] = '' if translated_value is None else str(translated_value)
            except ValueError:
                continue
        else:
            cell_updates.setdefault(sheet_name, {})[second_part] = translated_value

    for sheet_name, updates in cell_updates.items():
        sheet_path = sheet_map.get(sheet_name)
        if not sheet_path or sheet_path not in files:
            continue
        sheet_root = _etree.fromstring(files[sheet_path])
        for cell_ref, value in updates.items():
            _xlsx_set_cell_inline_text(sheet_root, cell_ref, value)
        files[sheet_path] = _etree.tostring(sheet_root, xml_declaration=True, encoding='UTF-8', standalone=True)

    if shape_updates:
        drawing_map = _xlsx_sheet_drawing_map_from_files(files, sheet_map)
        for sheet_name, drawing_paths in drawing_map.items():
            wanted = {
                idx: val
                for (sn, idx), val in shape_updates.items()
                if sn == sheet_name
            }
            if not wanted:
                continue

            for drawing_path in drawing_paths:
                if drawing_path not in files:
                    continue
                drawing_root = _etree.fromstring(files[drawing_path])
                for shape_idx, sp in enumerate(_collect_sp_elements(drawing_root), start=1):
                    if shape_idx in wanted:
                        _set_sp_text(sp, wanted[shape_idx])
                files[drawing_path] = _etree.tostring(
                    drawing_root,
                    xml_declaration=True,
                    encoding='UTF-8',
                    standalone=True,
                )

    with zipfile.ZipFile(output_filepath, 'w') as z_out:
        written = set()
        for name in order:
            content = files.get(name)
            if content is None:
                continue
            original = infos[name]
            zi = zipfile.ZipInfo(name, original.date_time)
            zi.compress_type = original.compress_type
            zi.external_attr = original.external_attr
            zi.create_system = original.create_system
            z_out.writestr(zi, content)
            written.add(name)

        for name, content in files.items():
            if name in written:
                continue
            z_out.writestr(name, content)


def extract_text_from_docx(filepath):
    """
    Tr√≠ch xu·∫•t text t·ª´ file DOCX, bao g·ªìm paragraphs, tables, headers, footers
    Tr·∫£ v·ªÅ dictionary v·ªõi format:
    - Paragraphs: {"ParagraphX": "Content"}
    - Tables: {"TableX!RyC z": "Content"}
    - Headers: {"Header_SectionX!ParagraphY": "Content"}
    - Footers: {"Footer_SectionX!ParagraphY": "Content"}
    """
    extracted_data = {}
    doc = Document(filepath)
    
    # 1. Tr√≠ch xu·∫•t text t·ª´ c√°c paragraph th√¥ng th∆∞·ªùng (kh√¥ng trong table)
    paragraph_idx = 0
    for para in doc.paragraphs:
        text_content = para.text.strip()
        if text_content:  # Ch·ªâ l·∫•y paragraph kh√¥ng r·ªóng
            paragraph_idx += 1
            key = f"Paragraph{paragraph_idx}"
            extracted_data[key] = text_content
    
    # 2. Tr√≠ch xu·∫•t text t·ª´ c√°c b·∫£ng
    for table_idx, table in enumerate(doc.tables, start=1):
        for row_idx, row in enumerate(table.rows, start=1):
            for col_idx, cell in enumerate(row.cells, start=1):
                text_content = cell.text.strip()
                if text_content:
                    key = f"Table{table_idx}!R{row_idx}C{col_idx}"
                    extracted_data[key] = text_content
    
    # 3. Tr√≠ch xu·∫•t text t·ª´ headers
    for section_idx, section in enumerate(doc.sections, start=1):
        header = section.header
        for para_idx, para in enumerate(header.paragraphs, start=1):
            text_content = para.text.strip()
            if text_content:
                key = f"Header_Section{section_idx}!Paragraph{para_idx}"
                extracted_data[key] = text_content
        
        # Tr√≠ch xu·∫•t t·ª´ table trong header (n·∫øu c√≥)
        for table_idx, table in enumerate(header.tables, start=1):
            for row_idx, row in enumerate(table.rows, start=1):
                for col_idx, cell in enumerate(row.cells, start=1):
                    text_content = cell.text.strip()
                    if text_content:
                        key = f"Header_Section{section_idx}!Table{table_idx}!R{row_idx}C{col_idx}"
                        extracted_data[key] = text_content
    
    # 4. Tr√≠ch xu·∫•t text t·ª´ footers
    for section_idx, section in enumerate(doc.sections, start=1):
        footer = section.footer
        for para_idx, para in enumerate(footer.paragraphs, start=1):
            text_content = para.text.strip()
            if text_content:
                key = f"Footer_Section{section_idx}!Paragraph{para_idx}"
                extracted_data[key] = text_content
        
        # Tr√≠ch xu·∫•t t·ª´ table trong footer (n·∫øu c√≥)
        for table_idx, table in enumerate(footer.tables, start=1):
            for row_idx, row in enumerate(table.rows, start=1):
                for col_idx, cell in enumerate(row.cells, start=1):
                    text_content = cell.text.strip()
                    if text_content:
                        key = f"Footer_Section{section_idx}!Table{table_idx}!R{row_idx}C{col_idx}"
                        extracted_data[key] = text_content
    
    return extracted_data

def replace_text_keep_format_docx(paragraph, new_text):
    """
    Thay th·∫ø text trong paragraph c·ªßa Word nh∆∞ng gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng (font, m√†u, bold, italic...)
    Chi·∫øn l∆∞·ª£c:
    1. L∆∞u ƒë·ªãnh d·∫°ng c·ªßa run ƒë·∫ßu ti√™n
    2. X√≥a text c·ªßa t·∫•t c·∫£ runs
    3. G√°n text m·ªõi v√†o run ƒë·∫ßu ti√™n (gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng)
    """
    if not paragraph.runs:
        # Kh√¥ng c√≥ run n√†o, t·∫°o m·ªõi
        paragraph.text = new_text
        return
    
    # L∆∞u ƒë·ªãnh d·∫°ng c·ªßa run ƒë·∫ßu ti√™n
    first_run = paragraph.runs[0]
    
    # X√≥a text c·ªßa t·∫•t c·∫£ runs
    for run in paragraph.runs:
        run.text = ""
    
    # G√°n text m·ªõi v√†o run ƒë·∫ßu ti√™n (gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng)
    first_run.text = new_text

def inject_text_to_docx(filepath, json_data):
    """
    N·∫°p text ƒë√£ d·ªãch v√†o file DOCX
    Gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng (font, m√†u, size, bold, italic...)
    """
    doc = Document(filepath)
    
    for key, translated_value in json_data.items():
        try:
            # 1. X·ª≠ l√Ω Paragraph th√¥ng th∆∞·ªùng: "ParagraphX"
            if key.startswith('Paragraph') and '!' not in key:
                para_num = int(key.replace('Paragraph', ''))
                # ƒê·∫øm l·∫°i c√°c paragraph kh√¥ng r·ªóng ƒë·ªÉ map ƒë√∫ng index
                current_para_idx = 0
                for para in doc.paragraphs:
                    if para.text.strip():  # Ch·ªâ ƒë·∫øm paragraph kh√¥ng r·ªóng
                        current_para_idx += 1
                        if current_para_idx == para_num:
                            replace_text_keep_format_docx(para, translated_value)
                            break
            
            # 2. X·ª≠ l√Ω Table: "TableX!RyCz"
            elif key.startswith('Table') and '!' in key and not key.startswith('Header_') and not key.startswith('Footer_'):
                parts = key.split('!')
                if len(parts) != 2:
                    continue
                
                # Parse table index
                table_part = parts[0]
                table_idx = int(table_part.replace('Table', '')) - 1
                
                if table_idx >= len(doc.tables):
                    continue
                
                table = doc.tables[table_idx]
                
                # Parse cell position: "R2C3"
                cell_part = parts[1]
                if not cell_part.startswith('R'):
                    continue
                
                cell_parts = cell_part.replace('R', '').split('C')
                if len(cell_parts) != 2:
                    continue
                
                row_idx = int(cell_parts[0]) - 1
                col_idx = int(cell_parts[1]) - 1
                
                if row_idx < len(table.rows) and col_idx < len(table.rows[row_idx].cells):
                    cell = table.rows[row_idx].cells[col_idx]
                    # Thay th·∫ø text trong paragraph ƒë·∫ßu ti√™n c·ªßa cell
                    if cell.paragraphs:
                        replace_text_keep_format_docx(cell.paragraphs[0], translated_value)
            
            # 3. X·ª≠ l√Ω Header: "Header_SectionX!ParagraphY" ho·∫∑c "Header_SectionX!TableY!RzCw"
            elif key.startswith('Header_Section'):
                parts = key.split('!')
                if len(parts) < 2:
                    continue
                
                # Parse section index
                section_part = parts[0].replace('Header_Section', '')
                section_idx = int(section_part) - 1
                
                if section_idx >= len(doc.sections):
                    continue
                
                header = doc.sections[section_idx].header
                
                # Check if it's a paragraph or table
                if parts[1].startswith('Paragraph'):
                    para_num = int(parts[1].replace('Paragraph', ''))
                    para_idx = 0
                    for para in header.paragraphs:
                        if para.text.strip():
                            para_idx += 1
                            if para_idx == para_num:
                                replace_text_keep_format_docx(para, translated_value)
                                break
                
                elif parts[1].startswith('Table') and len(parts) == 3:
                    # Header table cell
                    table_idx = int(parts[1].replace('Table', '')) - 1
                    if table_idx >= len(header.tables):
                        continue
                    
                    table = header.tables[table_idx]
                    cell_part = parts[2]
                    if not cell_part.startswith('R'):
                        continue
                    
                    cell_parts = cell_part.replace('R', '').split('C')
                    if len(cell_parts) != 2:
                        continue
                    
                    row_idx = int(cell_parts[0]) - 1
                    col_idx = int(cell_parts[1]) - 1
                    
                    if row_idx < len(table.rows) and col_idx < len(table.rows[row_idx].cells):
                        cell = table.rows[row_idx].cells[col_idx]
                        if cell.paragraphs:
                            replace_text_keep_format_docx(cell.paragraphs[0], translated_value)
            
            # 4. X·ª≠ l√Ω Footer: "Footer_SectionX!ParagraphY" ho·∫∑c "Footer_SectionX!TableY!RzCw"
            elif key.startswith('Footer_Section'):
                parts = key.split('!')
                if len(parts) < 2:
                    continue
                
                # Parse section index
                section_part = parts[0].replace('Footer_Section', '')
                section_idx = int(section_part) - 1
                
                if section_idx >= len(doc.sections):
                    continue
                
                footer = doc.sections[section_idx].footer
                
                # Check if it's a paragraph or table
                if parts[1].startswith('Paragraph'):
                    para_num = int(parts[1].replace('Paragraph', ''))
                    para_idx = 0
                    for para in footer.paragraphs:
                        if para.text.strip():
                            para_idx += 1
                            if para_idx == para_num:
                                replace_text_keep_format_docx(para, translated_value)
                                break
                
                elif parts[1].startswith('Table') and len(parts) == 3:
                    # Footer table cell
                    table_idx = int(parts[1].replace('Table', '')) - 1
                    if table_idx >= len(footer.tables):
                        continue
                    
                    table = footer.tables[table_idx]
                    cell_part = parts[2]
                    if not cell_part.startswith('R'):
                        continue
                    
                    cell_parts = cell_part.replace('R', '').split('C')
                    if len(cell_parts) != 2:
                        continue
                    
                    row_idx = int(cell_parts[0]) - 1
                    col_idx = int(cell_parts[1]) - 1
                    
                    if row_idx < len(table.rows) and col_idx < len(table.rows[row_idx].cells):
                        cell = table.rows[row_idx].cells[col_idx]
                        if cell.paragraphs:
                            replace_text_keep_format_docx(cell.paragraphs[0], translated_value)
        
        except (ValueError, IndexError, AttributeError) as e:
            # B·ªè qua c√°c key kh√¥ng h·ª£p l·ªá
            continue
    
    return doc

@app.route('/login', methods=['GET', 'POST'])
def login():
    """Trang ƒëƒÉng nh·∫≠p"""
    if request.method == 'POST':
        password = request.form.get('password', '')
        correct_password = get_password()
        
        if password == correct_password:
            session.permanent = True
            session['logged_in'] = True
            session['session_id'] = create_session_id()
            
            # Cleanup old sessions khi ƒëƒÉng nh·∫≠p
            cleanup_old_sessions()
            
            return redirect(url_for('index'))
        else:
            return render_template('login.html', error='M·∫≠t kh·∫©u kh√¥ng ƒë√∫ng!')
    
    return render_template('login.html')

@app.route('/logout')
def logout():
    """ƒêƒÉng xu·∫•t"""
    # X√≥a folder c·ªßa session hi·ªán t·∫°i
    if 'session_id' in session:
        session_folder = os.path.join(app.config['UPLOAD_FOLDER'], session['session_id'])
        if os.path.exists(session_folder):
            shutil.rmtree(session_folder)
    
    session.clear()
    return redirect(url_for('login'))

@app.route('/')
@login_required
def index():
    """
    Trang ch·ªß hi·ªÉn th·ªã dashboard v·ªõi 2 ch·ª©c nƒÉng Extract v√† Inject
    """
    # Cleanup old sessions m·ªói khi load trang
    cleanup_old_sessions()
    return render_template('index.html')

@app.route('/api/languages', methods=['GET'])
@login_required
def get_languages():
    """
    Tr·∫£ v·ªÅ danh s√°ch ng√¥n ng·ªØ ƒë√≠ch t·ª´ file languages.json
    """
    languages_file = os.path.join(os.path.dirname(__file__), 'languages.json')
    try:
        with open(languages_file, 'r', encoding='utf-8') as f:
            languages = json.load(f)
        return jsonify(languages)
    except FileNotFoundError:
        # Fallback n·∫øu file kh√¥ng t·ªìn t·∫°i
        return jsonify([
            {"code": "ja", "name": "ti·∫øng Nh·∫≠t",  "label": "üáØüáµ Ti·∫øng Nh·∫≠t (Japanese)"},
            {"code": "en", "name": "ti·∫øng Anh",   "label": "üá∫üá∏ Ti·∫øng Anh (English)"},
            {"code": "vi", "name": "ti·∫øng Vi·ªát",  "label": "üáªüá≥ Ti·∫øng Vi·ªát (Vietnamese)"},
            {"code": "zh", "name": "ti·∫øng Trung", "label": "üá®üá≥ Ti·∫øng Trung (Chinese)"},
            {"code": "ko", "name": "ti·∫øng H√†n",   "label": "üá∞üá∑ Ti·∫øng H√†n (Korean)"}
        ])
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ==================== API: PROMPT TEMPLATES ====================

@app.route('/api/templates', methods=['GET'])
@login_required
def api_get_templates():
    """Tr·∫£ v·ªÅ danh s√°ch prompt templates cho ng√¥n ng·ªØ ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh"""
    lang = request.args.get('lang', 'default')
    return jsonify(load_templates(lang))

@app.route('/api/templates', methods=['POST'])
@login_required
def api_save_templates():
    """L∆∞u danh s√°ch prompt templates cho ng√¥n ng·ªØ ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh"""
    lang = request.args.get('lang', 'default')
    new_templates = request.json
    if not isinstance(new_templates, list):
        return jsonify({'error': 'D·ªØ li·ªáu ph·∫£i l√† array'}), 400
    try:
        try:
            with open(TEMPLATES_FILE, 'r', encoding='utf-8') as f:
                all_data = json.load(f)
            if isinstance(all_data, list):
                all_data = {'default': all_data}
        except Exception:
            all_data = {}
        all_data[lang] = new_templates
        with open(TEMPLATES_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/extract', methods=['POST'])
@login_required
def extract():
    """
    Ch·ª©c nƒÉng 1: Tr√≠ch xu·∫•t c√°c cell ch·ª©a string t·ª´ file Excel, PPTX ho·∫∑c DOCX
    B·ªè qua c√°c cell ch·ª©a s·ªë v√† c√¥ng th·ª©c (b·∫Øt ƒë·∫ßu b·∫±ng '=') trong Excel
    Tr·∫£ v·ªÅ file JSON v·ªõi format: {"SheetName!CellCoordinate": "Content"} ho·∫∑c {"SlideX!ShapeY": "Content"} ho·∫∑c {"ParagraphX": "Content"}
    """
    # Ki·ªÉm tra xem c√≥ file ƒë∆∞·ª£c upload kh√¥ng
    if 'file' not in request.files:
        return jsonify({'error': 'Kh√¥ng c√≥ file ƒë∆∞·ª£c upload'}), 400
    
    file = request.files['file']
    
    # Ki·ªÉm tra xem file c√≥ ƒë∆∞·ª£c ch·ªçn kh√¥ng
    if file.filename == '':
        return jsonify({'error': 'Kh√¥ng c√≥ file ƒë∆∞·ª£c ch·ªçn'}), 400
    
    # Ki·ªÉm tra ƒë·ªãnh d·∫°ng file
    if not allowed_file(file.filename):
        return jsonify({'error': 'Ch·ªâ ch·∫•p nh·∫≠n file .xlsx, .pptx ho·∫∑c .docx'}), 400
    
    try:
        # L·∫•y session folder
        session_folder = get_session_folder()
        
        # L∆∞u t√™n file g·ªëc (gi·ªØ nguy√™n ti·∫øng Nh·∫≠t, k√Ω t·ª± ƒë·∫∑c bi·ªát)
        original_filename = file.filename
        
        # L·∫•y extension t·ª´ t√™n file g·ªëc
        if '.' in original_filename:
            original_ext = original_filename.rsplit('.', 1)[1].lower()
        else:
            return jsonify({'error': 'T√™n file ph·∫£i c√≥ ƒëu√¥i m·ªü r·ªông (.xlsx ho·∫∑c .pptx)'}), 400
        
        # T·∫°o t√™n file t·∫°m an to√†n ho√†n to√†n t·ª´ timestamp (kh√¥ng d√πng t√™n g·ªëc)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_temp_filename = f"temp_{timestamp}.{original_ext}"
        filepath = os.path.join(session_folder, safe_temp_filename)
        file.save(filepath)
        
        # X√°c ƒë·ªãnh lo·∫°i file v√† tr√≠ch xu·∫•t
        file_ext = original_ext
        
        if file_ext == 'xlsx':
            # M·ªü file Excel b·∫±ng openpyxl
            workbook = load_workbook(filepath)
            
            # Dictionary ƒë·ªÉ l∆∞u k·∫øt qu·∫£
            extracted_data = {}
            
            # Duy·ªát qua t·∫•t c·∫£ c√°c sheet
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                
                # Duy·ªát qua t·∫•t c·∫£ c√°c cell trong sheet
                for row in sheet.iter_rows():
                    for cell in row:
                        # B·ªè qua cell r·ªóng
                        if cell.value is None:
                            continue
                        
                        # Ch·ªâ l·∫•y cell ch·ª©a string
                        if isinstance(cell.value, str):
                            # B·ªè qua c√¥ng th·ª©c (b·∫Øt ƒë·∫ßu b·∫±ng '=')
                            if not cell.value.startswith('='):
                                # T·∫°o key theo format "SheetName!CellCoordinate"
                                key = f"{sheet_name}!{cell.coordinate}"
                                extracted_data[key] = cell.value
            
            # Tr√≠ch xu·∫•t text t·ª´ shapes/objects (text-box) trong xlsx
            shapes_from_xlsx = extract_xlsx_shapes(filepath)
            extracted_data.update(shapes_from_xlsx)

            # ƒê√≥ng workbook
            workbook.close()
        
        elif file_ext == 'pptx':
            # Tr√≠ch xu·∫•t text t·ª´ PPTX
            extracted_data = extract_text_from_pptx(filepath)
        
        elif file_ext == 'docx':
            # Tr√≠ch xu·∫•t text t·ª´ DOCX
            extracted_data = extract_text_from_docx(filepath)
        
        # T√°ch d·ªØ li·ªáu th√†nh nhi·ªÅu file, m·ªói file 400 c·∫∑p key-value
        CHUNK_SIZE = 400
        data_items = list(extracted_data.items())
        total_items = len(data_items)
        num_files = (total_items + CHUNK_SIZE - 1) // CHUNK_SIZE  # L√†m tr√≤n l√™n
        
        # L·∫•y t√™n file g·ªëc kh√¥ng c√≥ extension (gi·ªØ nguy√™n ti·∫øng Nh·∫≠t)
        base_filename = os.path.splitext(original_filename)[0]
        
        # N·∫øu base_filename r·ªóng, d√πng t√™n m·∫∑c ƒë·ªãnh
        if not base_filename or base_filename.strip() == '':
            base_filename = f"file_{timestamp}"
        # T√™n safe cho filesystem (d√πng timestamp)
        safe_base_filename = f"extracted_{timestamp}"
        
        # T√™n folder trong ZIP (gi·ªØ nguy√™n ti·∫øng Nh·∫≠t)
        folder_name = f"{base_filename}_json_to_translate"
        # T√™n folder t·∫°m trong filesystem (d√πng safe filename)
        safe_folder_name = f"{safe_base_filename}_temp_{timestamp}"
        
        # T·∫°o th∆∞ m·ª•c t·∫°m ƒë·ªÉ ch·ª©a c√°c file JSON (d√πng t√™n safe cho filesystem)
        temp_dir = os.path.join(session_folder, safe_folder_name)
        os.makedirs(temp_dir, exist_ok=True)
        
        json_files = []
        json_display_names = []  # L∆∞u t√™n hi·ªÉn th·ªã v·ªõi ti·∫øng Nh·∫≠t
        
        # T·∫°o c√°c file JSON nh·ªè
        for i in range(num_files):
            start_idx = i * CHUNK_SIZE
            end_idx = min((i + 1) * CHUNK_SIZE, total_items)
            chunk_data = dict(data_items[start_idx:end_idx])
            
            # T√™n file hi·ªÉn th·ªã (gi·ªØ nguy√™n ti·∫øng Nh·∫≠t)
            json_display_name = f"{base_filename}_part{i+1:02d}_of_{num_files:02d}.json"
            json_display_names.append(json_display_name)
            
            # T√™n file an to√†n cho filesystem
            safe_json_filename = f"{safe_base_filename}_part{i+1:02d}.json"
            json_filepath = os.path.join(temp_dir, safe_json_filename)
            
            # L∆∞u d·ªØ li·ªáu v√†o file JSON v·ªõi encoding UTF-8
            with open(json_filepath, 'w', encoding='utf-8') as json_file:
                json.dump(chunk_data, json_file, ensure_ascii=False, indent=2)
            
            json_files.append(json_filepath)
        
        # ƒê·ªçc n·ªôi dung t·ª´ng file JSON ƒë·ªÉ tr·∫£ v·ªÅ cho frontend
        files_data = []
        for idx, json_filepath in enumerate(json_files):
            with open(json_filepath, 'r', encoding='utf-8') as f:
                files_data.append({
                    'name': json_display_names[idx],
                    'content': f.read()
                })
        
        # T·∫°o file ZIP ch·ª©a folder v√† c√°c file JSON
        zip_display_name = f"{base_filename}_json_to_translate.zip"  # T√™n hi·ªÉn th·ªã
        safe_zip_filename = f"{safe_base_filename}_json_{timestamp}.zip"  # T√™n file trong filesystem
        zip_filepath = os.path.join(session_folder, safe_zip_filename)
        
        # D√πng ZIP_STORED ƒë·ªÉ kh√¥ng n√©n file JSON
        with zipfile.ZipFile(zip_filepath, 'w', zipfile.ZIP_STORED) as zipf:
            for idx, json_filepath_item in enumerate(json_files):
                arcname = os.path.join(folder_name, json_display_names[idx])
                zipf.write(json_filepath_item, arcname)
        
        # L∆∞u th√¥ng tin ZIP v√†o session ƒë·ªÉ download sau
        session['extract_zip'] = {
            'path': zip_filepath,
            'display_name': zip_display_name,
            'input_path': filepath,
            'json_files': json_files,
            'temp_dir': temp_dir
        }
        
        # T√≠nh to√°n dedup data (g·ªôp keys c√≥ c√πng value)
        dedup_files, dedup_mapping, dedup_stats = build_dedup_data(extracted_data, CHUNK_SIZE)

        # L∆∞u dedup mapping v√†o session folder ƒë·ªÉ d√πng khi inject
        dedup_mapping_path = os.path.join(session_folder, 'dedup_mapping.json')
        with open(dedup_mapping_path, 'w', encoding='utf-8') as f:
            json.dump(dedup_mapping, f, ensure_ascii=False, indent=2)

        # Tr·∫£ v·ªÅ JSON response v·ªõi danh s√°ch file ƒë·ªÉ frontend hi·ªÉn th·ªã
        return jsonify({
            'success': True,
            'total_files': num_files,
            'total_items': total_items,
            'files': files_data,
            'zip_display_name': zip_display_name,
            'dedup_files': dedup_files,
            'dedup_stats': dedup_stats
        })
        
    except Exception as e:
        # X·ª≠ l√Ω l·ªói
        return jsonify({'error': f'L·ªói khi x·ª≠ l√Ω file: {str(e)}'}), 500

@app.route('/download-zip', methods=['GET'])
@login_required
def download_zip():
    """
    Serve file ZIP ƒë√£ ƒë∆∞·ª£c t·∫°o t·ª´ /extract.
    X√≥a t·∫•t c·∫£ file t·∫°m sau khi g·ª≠i xong.
    """
    zip_info = session.get('extract_zip')
    if not zip_info:
        return jsonify({'error': 'Kh√¥ng t√¨m th·∫•y file ZIP. Vui l√≤ng tr√≠ch xu·∫•t l·∫°i.'}), 404
    
    zip_filepath = zip_info.get('path')
    zip_display_name = zip_info.get('display_name', 'download.zip')
    
    if not zip_filepath or not os.path.exists(zip_filepath):
        return jsonify({'error': 'File ZIP kh√¥ng c√≤n t·ªìn t·∫°i. Vui l√≤ng tr√≠ch xu·∫•t l·∫°i.'}), 404
    
    # X√≥a th√¥ng tin ZIP trong session
    session.pop('extract_zip', None)
    
    # Tr·∫£ v·ªÅ file ZIP
    response = send_file(zip_filepath, mimetype='application/zip')
    response = set_download_headers(response, zip_display_name, 'download.zip')
    
    # X√≥a t·∫•t c·∫£ file t·∫°m sau khi g·ª≠i
    input_path = zip_info.get('input_path')
    json_files = zip_info.get('json_files', [])
    temp_dir = zip_info.get('temp_dir')
    
    @response.call_on_close
    def cleanup():
        import time
        import gc
        gc.collect()
        time.sleep(0.1)
        
        try:
            if zip_filepath and os.path.exists(zip_filepath):
                os.remove(zip_filepath)
        except Exception as e:
            print(f"Warning: Kh√¥ng th·ªÉ x√≥a ZIP: {e}")
        
        try:
            if input_path and os.path.exists(input_path):
                os.remove(input_path)
        except Exception as e:
            print(f"Warning: Kh√¥ng th·ªÉ x√≥a input file: {e}")
        
        for jf in json_files:
            try:
                if os.path.exists(jf):
                    os.remove(jf)
            except Exception as e:
                print(f"Warning: Kh√¥ng th·ªÉ x√≥a JSON file: {e}")
        
        try:
            if temp_dir and os.path.exists(temp_dir):
                os.rmdir(temp_dir)
        except Exception as e:
            print(f"Warning: Kh√¥ng th·ªÉ x√≥a temp dir: {e}")
    
    return response

@app.route('/inject', methods=['POST'])
@login_required
def inject():
    """
    Ch·ª©c nƒÉng 2: N·∫°p d·ªØ li·ªáu t·ª´ file JSON ƒë√£ d·ªãch v√†o file Excel, PPTX ho·∫∑c DOCX g·ªëc
    Gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng, m√†u s·∫Øc c·ªßa file g·ªëc
    H·ªó tr·ª£ nhi·ªÅu file JSON ri√™ng l·∫ª ho·∫∑c file ZIP ch·ª©a nhi·ªÅu file JSON
    """
    # Ki·ªÉm tra xem c√≥ file ƒë∆∞·ª£c upload kh√¥ng
    if 'excel_file' not in request.files:
        return jsonify({'error': 'C·∫ßn upload file Excel, PPTX ho·∫∑c DOCX'}), 400
    
    excel_file = request.files['excel_file']
    
    # L·∫•y pasted JSON data n·∫øu c√≥
    pasted_json_data = request.form.get('pasted_json_data', None)
    
    # Ki·ªÉm tra xem c√≥ file JSON ƒë∆∞·ª£c upload ho·∫∑c c√≥ pasted JSON kh√¥ng
    json_files = request.files.getlist('json_files') if 'json_files' in request.files else []
    
    # Ki·ªÉm tra xem c√≥ √≠t nh·∫•t m·ªôt ngu·ªìn JSON
    has_json_files = len(json_files) > 0 and any(f.filename != '' for f in json_files)
    has_pasted_json = pasted_json_data is not None and pasted_json_data.strip() != ''
    
    if not has_json_files and not has_pasted_json:
        return jsonify({'error': 'C·∫ßn upload √≠t nh·∫•t 1 file JSON/ZIP ho·∫∑c paste JSON'}), 400
    
    # Ki·ªÉm tra xem file excel c√≥ ƒë∆∞·ª£c ch·ªçn kh√¥ng
    if excel_file.filename == '':
        return jsonify({'error': 'C·∫ßn ch·ªçn file Excel/PowerPoint/Word'}), 400
    
    # Ki·ªÉm tra ƒë·ªãnh d·∫°ng file
    if not allowed_file(excel_file.filename):
        return jsonify({'error': 'File ph·∫£i c√≥ ƒë·ªãnh d·∫°ng .xlsx, .pptx ho·∫∑c .docx'}), 400
    
    try:
        # L·∫•y session folder
        session_folder = get_session_folder()
        
        # L∆∞u t√™n file g·ªëc (gi·ªØ nguy√™n ti·∫øng Nh·∫≠t, k√Ω t·ª± ƒë·∫∑c bi·ªát)
        original_excel_filename = excel_file.filename
        
        # L·∫•y extension t·ª´ t√™n file g·ªëc
        if '.' in original_excel_filename:
            original_ext = original_excel_filename.rsplit('.', 1)[1].lower()
        else:
            return jsonify({'error': 'T√™n file ph·∫£i c√≥ ƒëu√¥i m·ªü r·ªông (.xlsx ho·∫∑c .pptx)'}), 400
        
        # T·∫°o t√™n file t·∫°m an to√†n ho√†n to√†n t·ª´ timestamp
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_temp_filename = f"temp_{timestamp}.{original_ext}"
        excel_filepath = os.path.join(session_folder, safe_temp_filename)
        excel_file.save(excel_filepath)
        
        # ƒê·ªçc v√† g·ªôp d·ªØ li·ªáu JSON t·ª´ t·∫•t c·∫£ c√°c file
        json_data = {}
        temp_files = []
        
        for json_file in json_files:
            if json_file.filename == '':
                continue
                
            json_filename = json_file.filename.lower()
            is_zip = json_filename.endswith('.zip')
            is_json = json_filename.endswith('.json')
            
            if not (is_zip or is_json):
                continue
            
            if is_zip:
                # X·ª≠ l√Ω file ZIP
                temp_zip_filename = f"temp_{timestamp}_{secure_filename(json_file.filename)}"
                zip_filepath = os.path.join(session_folder, temp_zip_filename)
                json_file.save(zip_filepath)
                temp_files.append(zip_filepath)
                
                # Gi·∫£i n√©n v√† ƒë·ªçc t·∫•t c·∫£ c√°c file JSON
                with zipfile.ZipFile(zip_filepath, 'r') as zipf:
                    for file_info in zipf.namelist():
                        if file_info.lower().endswith('.json'):
                            with zipf.open(file_info) as f:
                                try:
                                    content = f.read().decode('utf-8')
                                    chunk_data = json.loads(content)
                                    json_data.update(chunk_data)
                                except UnicodeDecodeError:
                                    # Th·ª≠ v·ªõi encoding kh√°c
                                    f.seek(0)
                                    content = f.read().decode('utf-8-sig')
                                    chunk_data = json.loads(content)
                                    json_data.update(chunk_data)
            else:
                # X·ª≠ l√Ω file JSON ƒë∆°n l·∫ª
                try:
                    json_content = json_file.stream.read().decode('utf-8')
                    chunk_data = json.loads(json_content)
                    json_data.update(chunk_data)
                except UnicodeDecodeError:
                    # Th·ª≠ v·ªõi encoding kh√°c n·∫øu UTF-8 th·∫•t b·∫°i
                    json_file.stream.seek(0)
                    json_content = json_file.stream.read().decode('utf-8-sig')
                    chunk_data = json.loads(json_content)
                    json_data.update(chunk_data)
                except json.JSONDecodeError as e:
                    return jsonify({'error': f'File JSON "{json_file.filename}" kh√¥ng h·ª£p l·ªá: {str(e)}'}), 400
        
        # X·ª≠ l√Ω pasted JSON data
        if pasted_json_data:
            try:
                pasted_data_list = json.loads(pasted_json_data)
                
                # pasted_data_list l√† danh s√°ch c√°c JSON objects
                if isinstance(pasted_data_list, list):
                    for idx, pasted_obj in enumerate(pasted_data_list):
                        if isinstance(pasted_obj, dict):
                            json_data.update(pasted_obj)
                        else:
                            return jsonify({'error': f'Pasted JSON #{idx + 1} kh√¥ng ph·∫£i l√† object'}), 400
                else:
                    return jsonify({'error': 'Pasted JSON data ph·∫£i l√† danh s√°ch c√°c objects'}), 400
                    
            except json.JSONDecodeError as e:
                return jsonify({'error': f'Pasted JSON kh√¥ng h·ª£p l·ªá: {str(e)}'}), 400

        # M·ªü r·ªông dedup keys n·∫øu c√≥ (khi user d·ªãch t·ª´ dedup JSON)
        if any(k.startswith('dedup_') for k in json_data):
            json_data = expand_dedup_data(json_data, session_folder)

        # X√°c ƒë·ªãnh lo·∫°i file v√† n·∫°p d·ªØ li·ªáu (d√πng t√™n file g·ªëc)
        file_ext = original_excel_filename.rsplit('.', 1)[1].lower()
        
        if file_ext == 'xlsx':
            # T·∫°o t√™n file output
            base_filename = os.path.splitext(original_excel_filename)[0]  # T√™n g·ªëc v·ªõi ti·∫øng Nh·∫≠t
            
            output_display_name = f"{base_filename}_translated.xlsx"  # T√™n hi·ªÉn th·ªã
            safe_output_filename = f"output_{timestamp}.xlsx"  # T√™n file trong filesystem
            output_filepath = os.path.join(session_folder, safe_output_filename)

            # ZIP-level patch: n·∫°p c·∫£ cell + shape tr·ª±c ti·∫øp tr√™n package g·ªëc
            inject_xlsx_shapes(excel_filepath, output_filepath, json_data)
            
            output_mimetype = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        
        elif file_ext == 'pptx':
            # N·∫°p text v√†o PPTX
            prs = inject_text_to_pptx(excel_filepath, json_data)
            
            # T·∫°o t√™n file output
            base_filename = os.path.splitext(original_excel_filename)[0]  # T√™n g·ªëc v·ªõi ti·∫øng Nh·∫≠t
            
            output_display_name = f"{base_filename}_translated.pptx"  # T√™n hi·ªÉn th·ªã
            safe_output_filename = f"output_{timestamp}.pptx"  # T√™n file trong filesystem
            output_filepath = os.path.join(session_folder, safe_output_filename)
            
            # L∆∞u file PPTX ƒë√£ ƒë∆∞·ª£c n·∫°p d·ªØ li·ªáu
            prs.save(output_filepath)
            del prs  # Gi·∫£i ph√≥ng memory
            
            output_mimetype = 'application/vnd.openxmlformats-officedocument.presentationml.presentation'
        
        elif file_ext == 'docx':
            # N·∫°p text v√†o DOCX
            doc = inject_text_to_docx(excel_filepath, json_data)
            
            # T·∫°o t√™n file output
            base_filename = os.path.splitext(original_excel_filename)[0]  # T√™n g·ªëc v·ªõi ti·∫øng Nh·∫≠t
            
            output_display_name = f"{base_filename}_translated.docx"  # T√™n hi·ªÉn th·ªã
            safe_output_filename = f"output_{timestamp}.docx"  # T√™n file trong filesystem
            output_filepath = os.path.join(session_folder, safe_output_filename)
            
            # L∆∞u file DOCX ƒë√£ ƒë∆∞·ª£c n·∫°p d·ªØ li·ªáu
            doc.save(output_filepath)
            del doc  # Gi·∫£i ph√≥ng memory
            
            output_mimetype = 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        
        # Tr·∫£ v·ªÅ file ƒë√£ ƒë∆∞·ª£c n·∫°p d·ªØ li·ªáu (d√πng t√™n hi·ªÉn th·ªã)
        response = send_file(
            output_filepath,
            mimetype=output_mimetype
        )
        
        default_ascii_name = 'download.docx' if file_ext == 'docx' else ('download.pptx' if file_ext == 'pptx' else 'download.xlsx')
        response = set_download_headers(response, output_display_name, default_ascii_name)
        
        # X√≥a t·∫•t c·∫£ file t·∫°m sau khi g·ª≠i response
        @response.call_on_close
        def cleanup():
            import time
            import gc
            
            # Force garbage collection ƒë·ªÉ gi·∫£i ph√≥ng file handles
            gc.collect()
            time.sleep(0.1)  # Delay nh·ªè ƒë·ªÉ ƒë·∫£m b·∫£o file ƒë∆∞·ª£c gi·∫£i ph√≥ng
            
            # X√≥a file output
            try:
                if os.path.exists(output_filepath):
                    os.remove(output_filepath)
            except Exception as e:
                print(f"Warning: Kh√¥ng th·ªÉ x√≥a output file: {e}")
            
            # X√≥a file Excel/PPTX/DOCX t·∫°m
            try:
                if os.path.exists(excel_filepath):
                    os.remove(excel_filepath)
            except Exception as e:
                print(f"Warning: Kh√¥ng th·ªÉ x√≥a file t·∫°m: {e}")
            
            # X√≥a t·∫•t c·∫£ file ZIP t·∫°m
            for temp_file in temp_files:
                try:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                except Exception as e:
                    print(f"Warning: Kh√¥ng th·ªÉ x√≥a file ZIP t·∫°m: {e}")
        
        return response
        
    except Exception as e:
        # X·ª≠ l√Ω l·ªói
        return jsonify({'error': f'L·ªói khi x·ª≠ l√Ω file: {str(e)}'}), 500

@app.route('/clear-uploads', methods=['POST'])
@login_required
def clear_uploads():
    """
    X√≥a t·∫•t c·∫£ file trong th∆∞ m·ª•c session hi·ªán t·∫°i
    """
    try:
        session_folder = get_session_folder()
        
        # Ki·ªÉm tra xem th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
        if not os.path.exists(session_folder):
            return jsonify({'success': True, 'message': 'Kh√¥ng c√≥ file n√†o ƒë·ªÉ x√≥a'}), 200
        
        # ƒê·∫øm s·ªë file ƒë√£ x√≥a
        deleted_count = 0
        
        # Duy·ªát qua t·∫•t c·∫£ file trong session folder
        for item in os.listdir(session_folder):
            item_path = os.path.join(session_folder, item)
            
            try:
                if os.path.isfile(item_path):
                    # X√≥a file
                    os.remove(item_path)
                    deleted_count += 1
                elif os.path.isdir(item_path):
                    # X√≥a th∆∞ m·ª•c con v√† t·∫•t c·∫£ n·ªôi dung b√™n trong
                    shutil.rmtree(item_path)
                    deleted_count += 1
            except Exception as e:
                print(f"Kh√¥ng th·ªÉ x√≥a {item_path}: {str(e)}")
        
        return jsonify({
            'success': True,
            'message': f'ƒê√£ x√≥a th√†nh c√¥ng {deleted_count} file trong phi√™n c·ªßa b·∫°n',
            'deleted_count': deleted_count
        }), 200
        
    except Exception as e:
        return jsonify({'error': f'L·ªói khi x√≥a file: {str(e)}'}), 500

# ƒê·∫£m b·∫£o th∆∞ m·ª•c uploads t·ªìn t·∫°i
if not os.path.exists(app.config['UPLOAD_FOLDER']):
    os.makedirs(app.config['UPLOAD_FOLDER'])


# ==================== IMAGE TRANSLATION ====================
ALLOWED_IMAGE_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'webp', 'bmp'}

def allowed_image(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_IMAGE_EXTENSIONS


@app.route('/img-translate/upload', methods=['POST'])
@login_required
def img_translate_upload():
    """Upload ·∫£nh v√†o session folder, tr·∫£ v·ªÅ filename."""
    if 'image' not in request.files:
        return jsonify({'error': 'Kh√¥ng t√¨m th·∫•y file ·∫£nh'}), 400
    file = request.files['image']
    if file.filename == '':
        return jsonify({'error': 'Ch∆∞a ch·ªçn file'}), 400
    if not allowed_image(file.filename):
        return jsonify({'error': 'ƒê·ªãnh d·∫°ng kh√¥ng h·ªó tr·ª£. Ch·ªâ nh·∫≠n: PNG, JPG, JPEG, GIF, WEBP, BMP'}), 400

    session_folder = get_session_folder()
    filename = secure_filename(file.filename)
    ts = datetime.now().strftime('%H%M%S')
    name, ext = os.path.splitext(filename)
    safe_filename = f"img_{ts}_{name[:30]}{ext}"
    filepath = os.path.join(session_folder, safe_filename)
    file.save(filepath)
    return jsonify({'success': True, 'filename': safe_filename}), 200


@app.route('/img-translate/image/<filename>', methods=['GET'])
@login_required
def img_translate_serve(filename):
    """Serve ·∫£nh ƒë√£ upload t·ª´ session folder."""
    session_folder = get_session_folder()
    safe = secure_filename(filename)
    filepath = os.path.join(session_folder, safe)
    if not os.path.exists(filepath):
        return jsonify({'error': 'File kh√¥ng t·ªìn t·∫°i'}), 404
    return send_file(filepath)


@app.route('/img-translate/prompt', methods=['POST'])
@login_required
def img_translate_prompt():
    """Sinh Instruction Prompt y√™u c·∫ßu AI ph√¢n t√≠ch ·∫£nh v√† tr·∫£ JSON overlay."""
    data = request.get_json() or {}
    target_lang = data.get('target_lang', 'ti·∫øng Nh·∫≠t')
    source_lang = data.get('source_lang', '').strip()
    img_w = int(data.get('image_width', 0))
    img_h = int(data.get('image_height', 0))

    source_note = f" (ng√¥n ng·ªØ g·ªëc trong ·∫£nh: {source_lang})" if source_lang else ""

    if img_w > 0 and img_h > 0:
        dim_note = f"\n\nK√≠ch th∆∞·ªõc ·∫£nh CH√çNH X√ÅC: {img_w} √ó {img_h} pixel (r·ªông √ó cao).\n" \
                   f"‚Üí Quy ƒë·ªïi t·ªça ƒë·ªô pixel sang %: left_pct = pixel_x / {img_w} √ó 100, top_pct = pixel_y / {img_h} √ó 100\n" \
                   f"‚Üí Quy ƒë·ªïi k√≠ch th∆∞·ªõc sang %: width_pct = pixel_w / {img_w} √ó 100, height_pct = pixel_h / {img_h} √ó 100\n" \
                   f"‚Üí font_size_pct = chi·ªÅu_cao_font_pixel / {img_h} √ó 100"
        font_example = round(24 / img_h * 100, 2) if img_h else 2.5
        dim_font_note = f"(v√≠ d·ª•: ch·ªØ 24px trong ·∫£nh {img_h}px cao ‚Üí font_size_pct = {font_example})"
    else:
        dim_note = ""
        font_example = 2.5
        dim_font_note = "(v√≠ d·ª•: 2.5)"

    prompt = f"""B·∫°n l√† chuy√™n gia OCR v√† d·ªãch thu·∫≠t chuy√™n nghi·ªáp. T√¥i s·∫Ω g·ª≠i cho b·∫°n m·ªôt b·ª©c ·∫£nh{source_note}.{dim_note}

NHI·ªÜM V·ª§:
1. Nh·∫≠n di·ªán (OCR) T·∫§T C·∫¢ c√°c v√πng c√≥ vƒÉn b·∫£n trong ·∫£nh.
2. D·ªãch to√†n b·ªô sang {target_lang} m·ªôt c√°ch t·ª± nhi√™n, ch√≠nh x√°c.
3. V·ªõi m·ªói v√πng vƒÉn b·∫£n, x√°c ƒë·ªãnh C√ÅC GI√Å TR·ªä SAU:

   top_pct    : t·ªça ƒë·ªô m√©p TR√äN c·ªßa text box, t√≠nh b·∫±ng % chi·ªÅu CAO ·∫£nh (0 = tr√™n c√πng, 100 = d∆∞·ªõi c√πng)
   left_pct   : t·ªça ƒë·ªô m√©p TR√ÅI c·ªßa text box, t√≠nh b·∫±ng % chi·ªÅu R·ªòNG ·∫£nh (0 = tr√°i, 100 = ph·∫£i)
   width_pct  : chi·ªÅu R·ªòNG text box, % chi·ªÅu r·ªông ·∫£nh
   height_pct : chi·ªÅu CAO text box, % chi·ªÅu cao ·∫£nh
   bg_color   : m√£ HEX m√†u n·ªÅn TH·ª∞C T·∫æ ngay ph√≠a sau vƒÉn b·∫£n (ƒë·ªÉ che ch·ªØ c≈©)
   text_color : m√£ HEX m√†u ch·ªØ ph√π h·ª£p ƒë·ªÉ ƒë·ªçc ƒë∆∞·ª£c tr√™n bg_color
   font_size_pct : c·ª° ch·ªØ t√≠nh b·∫±ng % chi·ªÅu cao ·∫£nh {dim_font_note}
                   ‚Üí PH·∫¢I x·∫•p x·ªâ b·∫±ng chi·ªÅu cao th·ª±c t·∫ø c·ªßa 1 d√≤ng ch·ªØ trong ·∫£nh
                   ‚Üí KH√îNG ƒë∆∞·ª£c nh·ªè h∆°n 60% height_pct (n·∫øu block l√† 1 d√≤ng)
                   ‚Üí V·ªõi block nhi·ªÅu d√≤ng: font_size_pct ‚âà height_pct / s·ªë_d√≤ng √ó 0.8

‚ö†Ô∏è Y√äU C·∫¶U B·∫ÆT BU·ªòC:
- T·ªça ƒë·ªô ph·∫£i bao ph·ªß CH√çNH X√ÅC v√πng ch·ª©a vƒÉn b·∫£n, sai s·ªë kh√¥ng qu√° 1%
- C√°c box KH√îNG ƒë∆∞·ª£c ch·ªìng l√™n nhau (tr·ª´ khi text th·ª±c s·ª± ch·ªìng trong ·∫£nh)
- bg_color l·∫•y t·ª´ m√†u n·ªÅn th·ª±c trong ·∫£nh, KH√îNG ƒë·∫∑t m√†u t√πy √Ω
- Tr·∫£ v·ªÅ JSON THU·∫¶N T√öY ‚Äî KH√îNG gi·∫£i th√≠ch, KH√îNG b·ªçc markdown code block ```

FORMAT JSON TR·∫¢ V·ªÄ (gi·ªØ ƒë√∫ng c·∫•u tr√∫c n√†y):
{{
  "text_blocks": [
    {{
      "original": "VƒÉn b·∫£n g·ªëc trong ·∫£nh",
      "translated": "B·∫£n d·ªãch sang {target_lang}",
      "top_pct": 10.5,
      "left_pct": 5.2,
      "width_pct": 30.0,
      "height_pct": 5.0,
      "bg_color": "#FFFFFF",
      "text_color": "#000000",
      "font_size_pct": {font_example}
    }}
  ]
}}"""

    return jsonify({'prompt': prompt}), 200


if __name__ == '__main__':
    # Ch·∫°y ·ª©ng d·ª•ng Flask ·ªü ch·∫ø ƒë·ªô debug
    app.run(debug=True, host='0.0.0.0', port=5001)
